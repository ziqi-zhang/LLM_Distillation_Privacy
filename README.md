
# Membership and Memorization in LLM Knowledge Distillation

<!-- <a href="https://arxiv.org/abs/2402.03898"><img src="https://img.shields.io/badge/Paper-arXiv:2402.03898-Green"></a>
<a href=#bibtex><img src="https://img.shields.io/badge/Paper-BibTex-yellow"></a> -->

Pytorch implementation for the paper: \
**Membership and Memorization in LLM Knowledge Distillation** \
Ziqi Zhang, Ali Shahin Shamsabadi, Hanxiao Lu, Yifeng Cai, Hamed Haddadi


## Scripts

The scripts for membership inference on distilled models are in `scripts/model_family_6k`. The scripts of memorization is in `mia`. 

## Environment Configuration

Please see `README_distiLLM.md`.

## Acknowledgement
The code framework is based on [DistiLLM](https://github.com/jongwooko/distillm) and [MiniLLM](https://github.com/kuleshov/minillm)

